{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e440365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting txtai\n",
      "  Downloading txtai-5.5.1-py3-none-any.whl (169 kB)\n",
      "     -------------------------------------- 169.8/169.8 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting transformers>=4.22.0\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "     ---------------------------------------- 7.2/7.2 MB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.18.4 in e:\\anaconda\\lib\\site-packages (from txtai) (1.21.5)\n",
      "Requirement already satisfied: pyyaml>=5.3 in e:\\anaconda\\lib\\site-packages (from txtai) (6.0)\n",
      "Collecting faiss-cpu>=1.7.1.post2\n",
      "  Downloading faiss_cpu-1.7.4-cp39-cp39-win_amd64.whl (10.8 MB)\n",
      "     ---------------------------------------- 10.8/10.8 MB 2.9 MB/s eta 0:00:00\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-2.0.1-cp39-cp39-win_amd64.whl (172.4 MB)\n",
      "     -------------------------------------- 172.4/172.4 MB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in e:\\anaconda\\lib\\site-packages (from torch>=1.6.0->txtai) (2.11.3)\n",
      "Requirement already satisfied: networkx in e:\\anaconda\\lib\\site-packages (from torch>=1.6.0->txtai) (2.8.4)\n",
      "Requirement already satisfied: filelock in e:\\anaconda\\lib\\site-packages (from torch>=1.6.0->txtai) (3.6.0)\n",
      "Requirement already satisfied: sympy in e:\\anaconda\\lib\\site-packages (from torch>=1.6.0->txtai) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in e:\\anaconda\\lib\\site-packages (from torch>=1.6.0->txtai) (4.3.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\anaconda\\lib\\site-packages (from transformers>=4.22.0->txtai) (4.64.1)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.1-cp39-cp39-win_amd64.whl (263 kB)\n",
      "     ------------------------------------ 263.9/263.9 kB 704.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\anaconda\\lib\\site-packages (from transformers>=4.22.0->txtai) (21.3)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "     -------------------------------------- 236.8/236.8 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\anaconda\\lib\\site-packages (from transformers>=4.22.0->txtai) (2022.7.9)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 3.5/3.5 MB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in e:\\anaconda\\lib\\site-packages (from transformers>=4.22.0->txtai) (2.28.1)\n",
      "Requirement already satisfied: fsspec in e:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=4.22.0->txtai) (2022.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\anaconda\\lib\\site-packages (from packaging>=20.0->transformers>=4.22.0->txtai) (3.0.9)\n",
      "Requirement already satisfied: colorama in e:\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers>=4.22.0->txtai) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in e:\\anaconda\\lib\\site-packages (from jinja2->torch>=1.6.0->txtai) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\anaconda\\lib\\site-packages (from requests->transformers>=4.22.0->txtai) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\anaconda\\lib\\site-packages (from requests->transformers>=4.22.0->txtai) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda\\lib\\site-packages (from requests->transformers>=4.22.0->txtai) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda\\lib\\site-packages (from requests->transformers>=4.22.0->txtai) (3.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in e:\\anaconda\\lib\\site-packages (from sympy->torch>=1.6.0->txtai) (1.2.1)\n",
      "Installing collected packages: tokenizers, safetensors, faiss-cpu, torch, huggingface-hub, transformers, txtai\n",
      "Successfully installed faiss-cpu-1.7.4 huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 torch-2.0.1 transformers-4.30.2 txtai-5.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install txtai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21fd5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.13\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdfc0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai.embeddings import Embeddings\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a187c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = Embeddings({\n",
    "    \n",
    "    \"path\": \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82c2316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1349090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a65555ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Rafay\n",
      "[nltk_data]     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Rafay\n",
      "[nltk_data]     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96f79adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e182af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_documents = [preprocess_text(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71983c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fd881be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    processed_query = preprocess_text(query)\n",
    "    query_vector = vectorizer.transform([processed_query])\n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    ranked_indices = similarity_scores.argsort()[::-1]\n",
    "    return [(documents[i], similarity_scores[i]) for i in ranked_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8991b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"This is a document\"\n",
    "results = search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfa7dd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: This document is the second document.\n",
      "Similarity Score: 0.78722297610404\n",
      "\n",
      "Document: Is this the first document?\n",
      "Similarity Score: 0.6292275146695526\n",
      "\n",
      "Document: This is the first document.\n",
      "Similarity Score: 0.6292275146695526\n",
      "\n",
      "Document: And this is the third one.\n",
      "Similarity Score: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result, score in results:\n",
    "    print(f\"Document: {result}\\nSimilarity Score: {score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ed6a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
